<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="generator" content="pandoc">
        <meta name="author" content="Peter Steinbach" />
            <title>GPU APIs</title>
    <meta name="description" content="GPU APIs">
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <!-- Bootstrap core CSS -->
    <link href="bootstrap/css/bootstrap.min.css" rel="stylesheet">
    
    <link rel="stylesheet" href="my_reveal.css"/>
        <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
    
        <link rel="stylesheet" href="reveal.js/lib/css/zenburn.css">
    
    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'reveal.js/css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>
    <!--[if lt IE 9]>
	<script src="reveal.js/lib/js/html5shiv.js"></script>
	<![endif]-->
          </head>
  <body>
        <div class="reveal">
      <div class="slides">

		<section>
	  <h1 class="title">GPU APIs</h1>
	  	  <p>
	    <b>Peter Steinbach</b><br>
	    <emph>(Max Planck Institute of Molecular Cell Biology and Genetics, Dresden)</emph><br>
	    <a href="mailto:steinbac@mpi-cbg.de">steinbac@mpi-cbg.de</a>
	    <br><br>
	  </p>
	  
	  <h3 class="date">TUD, January 15, 2017</h3>
	</section>
		
	<section><section id="massively-parallel-programming" class="titleslide slide level1" data-background="img/Titan_render.png" data-background-size="800px" style="margin-top: 20%"><h1>Massively Parallel Programming</h1></section><section id="yet-another-hype" class="slide level2">
<h2>Yet Another Hype?</h2>
<center>
<object type="image/svg+xml" data="data/201x_acc_fraction.svg" width="1300" border="0" style="background-color: #FFFFFF;">
</object>
<p>Data obtained from <a href="www.Top500.org">Top500.org</a></p>
</center>
</section><section id="vendor-options" class="slide level2">
<h2>Vendor Options</h2>
<!-- TODO: image origins -->
<div class="row">
<div class="col-xs-4">
<center>
<p>Nvidia Tesla<br />
<img src="img/nvidia_v100_x400.jpg" /></p>
GPU without Graphics
</center>
</div>
<div class="col-xs-4">
<center>
<p>AMD Radeon Instinct<br />
<img src="img/amd_radeoninstinct_mi25_x400.jpg" /></p>
GPU without Graphics
</center>
</div>
<div class="col-xs-4">
<center>
<p>Intel MIC<br />
<img src="img/xeon_phi_x400_deprecated.jpg" /></p>
Removed from Market in 2017
</center>
</div>
</div>
</section><section id="vendor-flag-ships" class="slide level2">
<h2>Vendor flag ships</h2>
<!-- TODO: image origins -->
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<em>Nvidia Tesla V100</em> <img src="img/nvidia_v100_x400.jpg" />
</center>
</div>
<div class="col-xs-6">
<center>
<em>AMD Radeon Instinct MI25</em> <img src="img/amd_radeoninstinct_mi25_x400.jpg" />
</center>
</div>
</div>
<p> </p>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<ul>
<li>GV100 chipsets</li>
<li>16 GB HBM2</li>
<li>900 GB/s to RAM</li>
<li>15 TFlops SP</li>
<li>7.5 TFlops DP</li>
</ul>
</center>
</div>
<div class="col-xs-6">
<center>
<ul>
<li>1x Vega</li>
<li>16 GB HBM2 RAM</li>
<li>700-900 GB/s to RAM</li>
<li>12.5 TFlops SP</li>
</ul>
</center>
</div>
</div>
<!-- http://www.theregister.co.uk/2012/05/18/inside_nvidia_kepler2_gk110_gpu_tesla/ -->
</section></section>
<section><section id="architecture" class="titleslide slide level1" data-background="img/nvidia_kepler_die_shot.jpg"><h1>Architecture</h1></section><section id="from-a-high-level" class="slide level2">
<h2>From a high level</h2>
<center>
<object type="image/svg+xml" data="figures/K40.svg" width="1400" border="0" style="background-color: #FFFFFF;">
</object>
</center>
<center>
Nvidia K40: 15 Streaming Multiprocessors (SMX), 12 GB of GDDR5 DRAM
</center>
</section><section id="kepler-smx-close-up" class="slide level2">
<h2>Kepler SMX Close-up</h2>
<div style="text-align: center;margin-top: 4%;">
<object type="image/svg+xml" data="figures/GK210_sm.svg" width="1600" border="0" style="background-color: #FFFFFF;">
</object>
</div>
<center>
CUDA core: 192 fp32 ops / clock <!-- (1/3 fp64 ops / clock) -->
</center>
</section><section id="simt-execution" class="slide level2">
<h2>SIMT Execution</h2>
<div class="row vertical-align">
<div class="col-xs-2">
<p><strong>Warp</strong></p>
</div>
<div class="col-xs-4">
<object type="image/svg+xml" data="figures/thread.svg" height="200" border="0">
</object>
</div>
<div class="col-xs-8">
<ul>
<li>smallest unit of concurrency: <em>32 threads</em></li>
<li>thread = single CUDA core</li>
<li>all threads execute same program</li>
</ul>
</div>
</div>
<div class="fragment">
<div class="row vertical-align">
<div class="col-xs-2">
<p><strong>Block</strong></p>
</div>
<div class="col-xs-4">
<object type="image/svg+xml" data="figures/thread_block.svg" height="200" border="0" class="img-rounded">
</object>
</div>
<div class="col-xs-8">
<ul>
<li>can synchronize (barriers)</li>
<li>can exchange data (common &quot;shared&quot; memory, etc.)</li>
</ul>
</div>
</div>
</div>
<div class="fragment">
<div class="row vertical-align">
<div class="col-xs-2">
<p><strong>Grid</strong></p>
</div>
<div class="col-xs-4">
<object type="image/svg+xml" data="figures/grid_block.svg" height="200" border="0" class="img-rounded">
</object>
</div>
<div class="col-xs-8">
<ul>
<li>grids/blocks serve as work distribution/sharing mechanism on device (occupancy)</li>
<li>blocks dispatched to SMX (in turn run warps)</li>
</ul>
</div>
</div>
</div>
</section><section id="data-locality" class="slide level2">
<h2>Data Locality</h2>
<center>
<div class="row vertical-align">
<div class="col-xs-12">
<object type="image/svg+xml" data="figures/gpu_cpu_dichotomy.svg" width="1400" border="0" class="img-rounded">
</object>
</div>
</div>
<p><strong>Keep data put as long as possible!</strong></p>
</center>
</section><section id="summary-architecture" class="slide level2">
<h2>Summary Architecture</h2>
<center>
<ul>
<li><p><strong>GPUs are complicated beasts</strong></p></li>
<li><p><strong>massive parallel compute power</strong> (per Watt)</p></li>
<li><strong>any API needs to support the above</strong></li>
</ul>
</center>
</section></section>
<section><section id="apis-today" class="titleslide slide level1" data-background="img/1024px-San_Francisco_Haight_Str_Guitar_Shop.jpg" style="color: black; margin: 0;margin-top: -100px;"><h1>APIs today?</h1></section><section id="a-word-of-warning" class="slide level2">
<h2>A Word of Warning!</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<!-- https://pixabay.com/p-42657/?no_redirect -->
<img src="img/800x_warning-42657_1280.png" />
</center>
</div>
<div class="col-xs-6">
<div style="font-size: 1.5em">
<center>
<ul>
<li><p>32 threads is the minimum</p></li>
<li>good tools are rare and almost never portable
<center></li>
</ul>
<div class="fragment">
<center>
<p><strong>Use a Library!</strong></p>
</center>
</div>
</div>
</div>
</div>
</section><section id="baseline-example" class="slide level2">
<h2>Baseline Example</h2>
<center>
<pre><code>void vector_sum(std::vector&lt;float&gt;&amp; a, 
                float scale, const std::vector&lt;float&gt;&amp; b) { 
    for (int i=0; i&lt;a.size(); i++)  
        a[i] = a[i] + b[i]; 
} </code></pre>
<p><strong>Vector Sum</strong></p>
<p> </p>
<p>Example: <a href="https://github.com/UoB-HPC/BabelStream/blob/v3.3">BabelStream Benchmark</a></p>
</center>
</section><section id="cuda-overview" class="slide level2">
<h2>CUDA Overview</h2>
<div class="row vertical-align">
<div class="col-xs-8">
<center>
<strong>C</strong>ompute <strong>U</strong>nified <strong>D</strong>evice <strong>A</strong>rchitecture (<a href="https://developer.nvidia.com/cuda-zone">Nvidia CUDA Zone</a>)
</center>
</div>
<div class="col-xs-4">
<center>
<img src="img/NVIDIA-CUDA.jpg" />
</center>
</div>
</div>
<center>
<ul>
<li><p>freeware tool suite, gpu library package and low/high level API(s)</p></li>
<li><p>CUDA platform supports C and C++ with proprietary compiler</p></li>
<li><p>binaries run on Nvidia hardware only</p></li>
<li><p>source code split into host and device part</p>
<ul>
<li><p>host : C++11/14 and STL supported</p></li>
<li><p>device: subset of C++11/14 (no exceptions, no iostream, no virtual inheritance, no STL)</p></li>
</ul></li>
</ul>
</center>
</section><section id="simple-steps-in-cuda" class="slide level2">
<h2><a href="http://devblogs.nvidia.com/parallelforall/easy-introduction-cuda-c-and-c/">5 Simple Steps In CUDA</a></h2>
<p> </p>
<div style="font-size: 1.5em">
<center>
<ol type="1">
<li>Declare and allocate host and device memory.</li>
<li>Initialize host data.</li>
<li>Transfer data from the host to the device.</li>
<li>Execute one or more kernels (vector sum).</li>
<li>Transfer results from the device to the host.</li>
</ol>
<p> </p>
<p><a href="https://github.com/UoB-HPC/BabelStream/blob/v3.3/CUDAStream.cu#L22">BabelStream CUDA</a></p>
</center>
</div>
</section><section id="cuda-code-mem-init" class="slide level2">
<h2>CUDA Code: Mem Init</h2>
<pre class="cpp"><code>int main(/*..*/){//..
  std::vector&lt;float&gt; host_a(vsize,1.f);
  std::vector&lt;float&gt; host_b(vsize,2.f);

  float * device_a=nullptr, *device_b=nullptr;
  cudaMalloc(&amp;device_a, vsize_byte);
  cudaMalloc(&amp;device_b, vsize_byte);

  cudaMemcpy(device_a, &amp;host_a[0], vsize_byte,
             cudaMemcpyHostToDevice);
  cudaMemcpy(device_b, &amp;host_b[0], vsize_byte,
             cudaMemcpyHostToDevice);</code></pre>
</section><section id="cuda-code-compute" class="slide level2">
<h2>CUDA Code: Compute</h2>
<pre class="cpp"><code>//above main
__global__ void vector_sum(std::size_t _size,
                           float* _a, float* _b){
  std::size_t index = blockIdx.x*blockDim.x + threadIdx.x;
  if (index &lt; _size)
    _a[index] = _scale*_a[index] + _b[index];
}

//in main: dispatch to device
vector_sum&lt;&lt;&lt;(vsize+255)/256, 256&gt;&gt;&gt;(vsize,
                                     device_a,
                                     device_b);</code></pre>
</section><section id="cuda-code-mem-tx-clean-up" class="slide level2">
<h2>CUDA Code: Mem TX + Clean-up</h2>
<pre class="cpp"><code>  //transfer memory back
  cudaMemcpy(&amp;host_a[0], device_a, vsize_byte,
             cudaMemcpyDeviceToHost);

  //clean-up
  cudaFree(device_a);
  cudaFree(device_b);
  return 0;
}
</code></pre>
</section><section id="cuda-wrap-up" class="slide level2">
<h2>CUDA Wrap-up</h2>
<div class="row">
<div class="col-xs-6 text-success">
<center>
<ul>
<li><p><em>free and working</em></p></li>
<li><p>CUDA comes with a <em>ton of tools</em> (debugger, profiler, libraries, ...)</p></li>
<li><p>CUDA comes with a <em>ton of examples</em></p></li>
<li><p>very <em>flexible</em> (device instrinsics, locked memory handling, ...)</p></li>
<li><p><em>nVidia very active</em> in porting scientific applications</p></li>
<li><p><em>nVidia very active</em> C++ standardisation (Parallelism TS)</p></li>
</ul>
</center>
</div>
<div class="fragment">
<div class="col-xs-6 text-warning">
<center>
<ul>
<li><p>plain C runtime API on host (memory allocation, error handling, asynchronous calls, ...)</p></li>
<li><p>grid dispatch is error prone (code repetition in index calculation)</p></li>
<li><p>compiler is sometimes hard to come by (using boost, OpenMP interoperability)</p></li>
<li><p><code>__keyword__</code> disrupt design (redundancy, maintainability)</p></li>
</ul>
</center>
</div>
</div>
</div>
</section><section id="cuda-is-like-..." class="slide level2" data-background="img/1024px-Taylor415_acoustic.jpg">
<h2>CUDA is like ...</h2>
</section><section id="opencl" class="slide level2">
<h2>OpenCL</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<strong>Open C</strong>ompute <strong>L</strong>anguage (<a href="https://www.khronos.org/opencl/">khronos.org/opencl</a>)
</center>
</div>
<div class="col-xs-4 bg-primary">
<center>
<em>No Logo due to Apple's Copyright</em>
</center>
</div>
</div>
<p> </p>
<center>
<ul>
<li><p>open, royalty-free standard for cross-platform, parallel programming</p></li>
<li><p>designed to run on CPUs, GPUs, FPGAs, DSPs, ...</p></li>
<li><p>maintained by non-profit technology consortium Khronos Group</p></li>
<li><p>source code split into host and device part</p>
<ul>
<li><p>host : C/C++ based API (lower level than CUDA)</p></li>
<li><p>device: C99 (OpenCL &lt;2) or C11 derived language (<a href="https://www.khronos.org/registry/cl/sdk/2.0/docs/OpenCL-2.0-refcard.pdf">OpenCL 2.0</a>)</p></li>
</ul></li>
</ul>
</center>
</section><section id="standardisation" class="slide level2">
<h2>Standardisation</h2>
<center>
<img src="img/opencl-standardisation.jpg" />
</center>
<ul>
<li>OpenCL 2.2 out since May 16, 2017 (C++14 in kernels, SPIR-V 1.1 as intermediate language)</li>
</ul>
</section><section id="implementers" class="slide level2">
<h2>Implementers</h2>
<center>
<img src="img/opencl-companies.png" />
</center>
</section><section id="opencl-eco-system" class="slide level2">
<h2>OpenCL eco system</h2>
<center>
<img src="img/opencl-ecosystem.png" />
</center>
</section><section id="opencl-api-flow" class="slide level2">
<h2>OpenCL API flow</h2>
<center>
<figure>
<img src="img/opencl-APIflow.png" />
</figure>
<p> </p>
<p><a href="https://github.com/UoB-HPC/BabelStream/blob/v3.3/OCLStream.cpp#L95">Real World Example</a></p>
</center>
</section><section id="opencl-kernel" class="slide level2">
<h2>OpenCL Kernel</h2>
<pre class="cpp"><code>const char *kernelSource =                     &quot;\n&quot; \
&quot;__kernel void vecAdd(  __global float *a,      \n&quot; \
&quot;                       __global float *b,      \n&quot; \
&quot;                       __global float *c,      \n&quot; \
&quot;                       const unsigned int n)   \n&quot; \
&quot;{                                              \n&quot; \
&quot;    int id = get_global_id(0);                 \n&quot; \
&quot;                                               \n&quot; \
&quot;    //Make sure we do not go out of bounds     \n&quot; \
&quot;    if (id &lt; n)                                \n&quot; \
&quot;        c[id] = a[id] + b[id];                 \n&quot; \
&quot;}                                              \n&quot; \
                                               &quot;\n&quot; ;</code></pre>
<center>
from <a href="https://www.olcf.ornl.gov/tutorials/opencl-vector-addition/">www.olcf.ornl.gov</a>
</center>
</section><section id="opencl-is-like-..." class="slide level2">
<h2>OpenCL is like ...</h2>
<center>
<img src="img/Andy_McKee_January_2008.jpg" alt="by Kasra Ganjavi" />
</center>
</section><section id="opencl-translation-table" class="slide level2">
<h2>OpenCL Translation Table</h2>
<div class="row">
<div class="col-xs-6">
<p>OpenCL</p>
<ul>
<li><p>local memory</p></li>
<li><p>private memory</p></li>
</ul>
</div>
<div class="col-xs-6">
<p>CUDA</p>
<ul>
<li><p>shared memory</p></li>
<li><p>local memory</p></li>
</ul>
</div>
</div>
<div class="fragment">
<div class="row">
<div class="col-xs-6">
<ul>
<li><p>NDRange (index space)</p></li>
<li><p>work group</p></li>
<li><p>work item</p></li>
</ul>
</div>
<div class="col-xs-6">
<ul>
<li><p>grid</p></li>
<li><p>block</p></li>
<li><p>thread</p></li>
</ul>
</div>
</div>
</div>
</section><section id="boost.compute" class="slide level2">
<h2><a href="https://github.com/boostorg/compute">Boost.Compute</a></h2>
<center>
<p>OpenCL wrapper enabling vendor independent parallel algorithms</p>
</center>
<pre class="cpp"><code>    compute::device gpu = compute::system::default_device();
    compute::context ctx(gpu);
    compute::command_queue queue(ctx, gpu);

    compute::vector&lt;float&gt; device_a(a.size(), ctx);//etc..
    compute::copy(host_a.begin(), host_a.end(),
        device_a.begin(), queue);//etc..

    compute::transform(device_a.begin(),device_a.end(),
        device_a.begin(),compute::add&lt;float&gt;(),queue);</code></pre>
</section><section id="opencl-summary" class="slide level2">
<h2>OpenCL Summary</h2>
<div class="row">
<div class="col-xs-6 text-success">
<center>
<ul>
<li><p><em>free</em></p></li>
<li><p>CPU+GPU+...</p></li>
<li><p>vendor independent, standardised API</p></li>
<li><p>write kernels once, dispatch anywhere (performance portability)</p></li>
<li><p>backwards compatible (long term support)</p></li>
<li><p>wrappers look promising (boost.compute)</p></li>
</ul>
</center>
</div>
<div class="fragment">
<div class="col-xs-6 text-warning">
<center>
<ul>
<li><p>plain C runtime API on host + device (memory allocation, error handling, asynchronous calls, ...)</p></li>
<li><p>more explicit than CUDA</p></li>
<li><p>no good tooling (debuggers, profilers, ...)</p></li>
<li><p>kernels as strings? (runtime reveals bugs)</p></li>
<li><p>kernels written in C99 originally</p></li>
</ul>
</center>
</div>
</div>
</div>
</section><section id="thrust" class="slide level2">
<h2>thrust</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<em>parallel algorithms library which resembles the C++ Standard Template Library (STL)</em>
</center>
</div>
<div class="col-xs-4">
<center>
<img src="img/thrust_logo.png" /> <a href="http://thrust.github.io/">thrust.github.io</a>
</center>
</div>
</div>
<p> </p>
<center>
<ul>
<li><p>open source (Apache v2 license)</p></li>
<li><p>interoperability with CUDA, TBB and OpenMP (possible backends)</p></li>
<li>high level interface compared to CUDA/OpenCL</li>
</ul>
</center>
</section><section id="thrust-code-functor" class="slide level2">
<h2>thrust Code: Functor</h2>
<pre class="cpp"><code>struct saxpy_functor :
public thrust::binary_function&lt;float,float,float&gt;
{
    const float a;
    saxpy_functor(float _a) : a(_a) {}

    __host__ __device__
    float operator()(const float&amp; x,
                     const float&amp; y
                    ) const {
            return a * x + y;
        }
};</code></pre>
</section><section id="thrust-code-gpu-dispatch" class="slide level2">
<h2>thrust Code: GPU dispatch</h2>
<pre class="cpp"><code>int main(//...){//..

  thrust::host_vector&lt;float&gt; host_a(N,1.f);
  thrust::host_vector&lt;float&gt; host_b(N,2.f);

  thrust::device_vector&lt;float&gt; dev_a = host_a;
  thrust::device_vector&lt;float&gt; dev_b = host_b;

  thrust::transform(dev_a.begin(),dev_a.end(),
                    dev_b.begin(),
                    dev_a.begin(),
                    saxpy_functor(scale));
}</code></pre>
</section><section id="thrust-wrap-up" class="slide level2">
<h2>thrust Wrap-up</h2>
<div class="row">
<div class="col-xs-6 text-success">
<center>
<ul>
<li><p>C++ and STL for GPUs (and CPUs)!</p></li>
<li><p>container and algorithm API well thought through</p></li>
<li><p>code becomes readable/maintainable (at least for a C++Dev)</p></li>
<li><p>algorithms can be dispatched from device kernels as well</p></li>
<li>many examples, active community</li>
</ul>
</center>
</div>
<div class="fragment">
<div class="col-xs-6 text-warning">
<center>
<ul>
<li><p>loss of flexibility:</p>
<ul>
<li><p>host-device i/o (pinned memory allocator considered experimental)</p></li>
<li><p>index information only available through kernel built-ins</p></li>
<li><p>grid distpatch of kernel by thrust library (occupancy)</p></li>
</ul></li>
<li><p>kernel optimisations = CUDA (<a href="https://nvlabs.github.io/cub/">CUB</a> library?)</p></li>
</ul>
</center>
</div>
</div>
</div>
</section><section id="thrust-is-like-..." class="slide level2">
<h2>thrust is like ...</h2>
<center>
<img src="img/baseguitar.jpg" alt="by axeplace.com" />
</center>
</section><section id="hcc" class="slide level2">
<h2>HCC</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<strong>H</strong>eterogenous <strong>C</strong>ompute <strong>C</strong>ompiler (<a href="https://bitbucket.org/multicoreware/hcc/wiki/Home">bitbucket.org/multicoreware/hcc</a>)
</center>
</div>
<div class="col-xs-4">
<center>
meant for APU <img src="img/apu_comic.gif" /><br />
<strong>A</strong>ll-purpose G<strong>PU</strong>s
</center>
</div>
</div>
<center>
<ul>
<li><p>single source C++ compiler (for CPU, GPU and some APU targets)</p></li>
<li><p>supports C++AMP 1.2, HC, OpenMP 4, C++1x</p></li>
</ul>
</section><section id="hcc-vector-sum-camp" class="slide level2">
<h2>HCC Vector Sum (C++AMP)</h2>
<pre class="cpp"><code>using namespace concurrency;

void amp_sum(vector&lt;float&gt;&amp; _va,
             const vector&lt;float&gt;&amp; _vb,
             float _scale){
         
  extent&lt;1&gt; ext_a(_va.size()),ext_b(_vb.size());

  array_view&lt;float, 1&gt; view_a(ext_a,_va);
  array_view&lt;const float, 1&gt; view_b(ext_b,_vb);</code></pre>
</section><section id="hcc-continued" class="slide level2">
<h2>HCC continued</h2>
<pre class="cpp"><code>  parallel_for_each(view_a.get_extent(),
            [=](index&lt;1&gt; idx) restrict(amp)
            {
              view_a[idx] = view_a[idx]*_scale + view_b[idx];
            }
            );

  view_a.synchronize();
}</code></pre>
</section><section id="hcc-wrap-up" class="slide level2">
<h2>HCC Wrap-up</h2>
<div class="row">
<div class="col-xs-6 text-success">
<center>
<ul>
<li><p>API focusses on problem-solving and flexibility</p></li>
<li><p>API appears to be lightweight (array views)</p></li>
<li><p>multiple binary backends (SPIR-V, OpenCL, ...)</p></li>
<li><p>multiple hardware backends (CPU, GPU, APU)</p></li>
<li><p>homogenous C++ source code</p></li>
<li><p>function continuations supported</p>
<pre><code>future1.then(future2)//.. </code></pre></li>
</ul>
</center>
</div>
<div class="fragment">
<div class="col-xs-6 text-warning">
<center>
<ul>
<li><p>young project, API still fluid (<code>concurrency::</code> =&gt; <code>hc::</code>)</p></li>
<li><p>no tooling yet (debugger, profiler, ...)</p></li>
<li><p>performance yield unclear</p></li>
<li><p>combined API for integrated and discrete GPUs</p></li>
<li><p>HSA/AMD road map unclear</p></li>
</ul>
</center>
</div>
</div>
</div>
</section><section id="hc-is-like-..." class="slide level2">
<h2>HC is like ...</h2>
<center>
<p><img src="img/Elektrofryingpan_landscape.jpg" /><br />
1932, by <a href="https://commons.wikimedia.org/wiki/File:Elektrofryingpan.jpg">Museum of Making Music at English Wikipedia</a></p>
</center>
</section><section id="pragma-based-approaches" class="slide level2">
<h2>Pragma based approaches</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<strong>Open M</strong>ulti-<strong>P</strong>rocessing (<a href="http://openmp.org/">openmp.org</a>)
</center>
</div>
<div class="col-xs-4">
<center>
<img src="img/OpenMP_logo.png" />
</center>
</div>
</div>
<pre class="cpp"><code>void vector_sum(int size, float scale,
                float * restrict a, float * restrict b) {
    #pragma omp target map(to:b[0:n], size, scale) map(a[0:n])
    {
        #pragma omp parallel for
        for (int i=0; i&lt;size; i++) {
            a[i] = a[i] * scale + b[i];
        }
    }
}</code></pre>
<center>
accelerator target since version 4.0 (<a href="https://gcc.gnu.org/wiki/Offloading">gcc 5.0+</a>, <a href="https://software.intel.com/en-us/intel-parallel-studio-xe">icc 16+</a>, <a href="http://www.pathscale.com/enzo">ENZO2016</a>)
</center>
</section><section id="pragmas-continued" class="slide level2">
<h2>Pragmas continued</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<strong>Open Acc</strong>elerator (<a href="http://openacc.org/">openacc.org</a>)
</center>
</div>
<div class="col-xs-4">
<center>
<img src="img/openacc_logo.jpg" />
</center>
</div>
</div>
<pre class="cpp"><code>void vector_sum(int size, float scale, float *a, float *b) {

    #pragma acc parallel copy(a[0:size]) copyin(b[0:size])
    #pragma acc loop
    for (int i = 0; i &lt; size; ++i)
        a[i] = scale*a[i] + b[i];
    
}</code></pre>
<center>
(partially available in <a href="https://gcc.gnu.org/wiki/Offloading">gcc 5.0+</a>, fully in <a href="https://www.pgroup.com/resources/accel.htm">pgi</a> &amp; <a href="http://www.pathscale.com/enzo">ENZO2016</a> compiler)
</center>
</section><section id="pragma-wrap-up" class="slide level2">
<h2>Pragma Wrap-up</h2>
<div class="row">
<div class="col-xs-6 text-success">
<center>
<ul>
<li><p>OpenMP is (already) a success story (why not OpenACC as well)</p></li>
<li><p>dream: one-line injection and code is fast</p></li>
<li><p>strong industrial support (tooling)</p></li>
<li><p>GPU: perfect fit for upgrading legacy code or prototyping</p></li>
</ul>
</center>
</div>
<div class="fragment">
<div class="col-xs-6 text-warning">
<center>
<ul>
<li><p>OpenMP works well on shared memory CPUs</p></li>
<li><p>(discrete) GPUs have different architecture than CPUs</p></li>
<li><p>language in a language ??</p></li>
<li><p>OpenACC, OpenMP dichotomy (will users/applications loose?)</p></li>
</ul>
</center>
</div>
</div>
</div>
</section><section id="pragmas-are-like-..." class="slide level2">
<h2>Pragmas are like ...</h2>
<center>
<p><img src="img/1024px-Ukulele-electro-acoustic.jpg" /><br />
by <a href="https://commons.wikimedia.org/wiki/File:Ukulele-electro-acoustic.JPG">Alno</a></p>
</center>
</section><section id="whats-next" class="slide level2">
<h2>What's next?</h2>
</section><section id="c17" class="slide level2">
<h2>C++17</h2>
<div class="row vertical-align">
<div class="col-xs-6">
<center>
<p>Published already:</p>
<a href="https://github.com/cplusplus/parallelism-ts">Parallelism TS</a>, <a href="https://github.com/cplusplus/concurrency-ts">Concurrency TS</a>
</center>
</div>
<div class="col-xs-4">
<center>
<a href="https://isocpp.org"><img src="img/logo-cpp.jpg" /></a>
</center>
</div>
</div>
<pre class="cpp"><code>transform(  std::experimental::parallel::par,
            std::begin(a), std::end(a),
            std::begin(b),
            std::begin(a)
            [&amp;](float&amp; a, const float&amp; b) {
                a = scale*a + b;
            });</code></pre>
<center>
<p><strong>vendors plan to support this with</strong></p>
<div style="width: 70%;align: center;">
<pre><code>std::parallel::cuda</code></pre>
</div>
</center>
</section><section id="c17-and-host-device-interaction" class="slide level2">
<h2>C++17 and host-device interaction</h2>
<div class="fragment">
<pre class="cpp"><code>future&lt;int&gt; f1 = copy_to_device();
future&lt;int&gt; f2 = f1.then([](future&lt;int&gt; f) {
                    future&lt;int&gt; f3 = start_compute();
                    return f3;
                    });
future&lt;int&gt; f3 = f3.then([](future&lt;int&gt; f){
                    return copy_to_host();
                    }
                    );</code></pre>
<center>
<p>  taken from concurrency TS</p>
<ul>
<li><p>better API to coordinate asynchronous transfers and computations</p></li>
<li><p>future: use <code>(a)wait/then</code> and friends to express data dependencies</p></li>
<li>support by compiler vendors needed</li>
</ul>
</center>
</div>
</section></section>
<section><section id="summary" class="titleslide slide level1"><h1>Summary</h1></section><section id="section" class="slide level2">
<h2></h2>
<center>
<ul>
<li><p>in production: almost dominated by C99 or C99-like APIs</p></li>
<li><p>on the horizon: performant, flexible and maintainable <strong>C++ APIs</strong> emerging (cub/thrust, boost.compute, sycl, ...)</p></li>
<li><p>GPUs architecture is complex: obtaining max. performance challenging</p></li>
</ul>
</center>
</section><section id="acknowledgements" class="slide level2">
<h2>Acknowledgements</h2>
<center>
<div class="row">
<div class="col-xs-4">
<p><a href="http://www.mpi-cbg.de">MPI CBG</a> / <a href="www.scionics.de">Scionics Computer Innovations GmbH</a></p>
</div>
<div class="col-xs-8 text-left">
<p><em>Robert Haase, Ian Henry, Benoit Lombardot, Jeff Oegema</em></p>
</div>
</div>
<div class="row">
<div class="col-xs-4">
<p><a href="http://ccoe-dresden.de/">GPU Center of Excellence</a></p>
</div>
<div class="col-xs-8 text-left">
<p><em>Guido Juckeland, Thomas Karnagel, René Widera, Erik Zenker</em></p>
</div>
</div>
<div class="row">
<div class="col-xs-4">
<p><a href="http://amd.com/">AMD</a>; <a href="http://www.multicorewareinc.com/">Multicoreware</a></p>
</div>
<div class="col-xs-8 text-left">
<p><em>Greg Stoner, Ben Sander, Chan SiuChi; Jack Chung</em></p>
</div>
</div>
<div class="row">
<div class="col-xs-4">
<p><a href="http://nvidia.com/">nVidia</a></p>
</div>
<div class="col-xs-8 text-left">
<p><em>Axel Köhler</em></p>
</div>
</div>
</center>
</section><section id="thank-you" class="slide level2">
<h2>Thank you!</h2>
<center>
<img src="img/Sleeping_students.jpg" />
</center>
</section><section id="section-1" class="slide level2" style="font-size: 1.5em">
<h2></h2>
<center>
<p><strong>For Questions, Comments, Complaints, Compliments, ... </strong></p>
<strong><a href="https://github.com/psteinb/gpu-lecture-APIs">github.com/psteinb/gpu-lecture-APIs</a></strong>
</center>
</section></section>
      </div>
    </div>


    <script src="reveal.js/lib/js/head.min.js"></script>
    <script src="reveal.js/js/reveal.js"></script>

    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
      controls: false,
      progress: true,
      history: true,
      center: true,
      
      slideNumber: true,
      // The "normal" size of the presentation, aspect ratio will be preserved
      // when the presentation is scaled to fit different resolutions. Can be
      // specified using percentage units.
      width: '1600',
      height: 1080,

      // Factor of the display size that should remain empty around the content
      margin: 0.01,

      // Bounds for smallest/largest possible scale to apply to content
      minScale: 0.2,
      maxScale: 1.5,

      theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
      transition: 'slide', // default/cube/page/concave/zoom/linear/fade/none

      // Optional libraries used to extend on reveal.js
      dependencies: [
      { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
            { src: 'reveal.js/plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
            { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
      { src: 'reveal.js/plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } },
      //          { src: 'reveal.js/plugin/search/search.js', async: true, condition: function() { return !!document.body.classList; }, }
      //          { src: 'reveal.js/plugin/remotes/remotes.js', async: true, condition: function() { return !!document.body.classList; } }
      ]});
    </script>
      </body>
</html>
